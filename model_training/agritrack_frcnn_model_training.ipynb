{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeZnvC34YrZE"
   },
   "source": [
    "## Agritrack - Faster R-CNN Sagemaker Model Training Code\n",
    "\n",
    "Note: This notebook assumes that there is a .zip dataset file at the root directory of your runtime for use in its execution as a dependency.\n",
    "\n",
    "Please assign the path to your .zip dataset to the **dataset_path** variable before executing the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsm76viLz6-p"
   },
   "source": [
    "Extract uploaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ktCHjt5Vz89L",
    "outputId": "ec346dc8-691f-4488-db08-f466b5fdcb36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dataset_5.zip\n",
      "   creating: extracted_dataset_5/sheep_video_02/\n",
      "  inflating: extracted_dataset_5/__MACOSX/._sheep_video_02  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/.DS_Store  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/._.DS_Store  \n",
      "   creating: extracted_dataset_5/sheep_video_02/test/\n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/._test  \n",
      "   creating: extracted_dataset_5/sheep_video_02/train/\n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/._train  \n",
      "   creating: extracted_dataset_5/sheep_video_02/val/\n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/._val  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00155_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00155_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00155_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00155_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00152_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00152_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00152_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00152_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00160_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00160_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00160_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00160_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00141_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00141_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00141_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00141_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00172_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00172_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00172_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00172_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00010_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00010_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/test/frame_00010_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/test/._frame_00010_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00155_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00155_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00155_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00155_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00160_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00160_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00160_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00160_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00021_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00021_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00160_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00160_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00160_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00160_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00021_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00021_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00172_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00172_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00172_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00172_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/.DS_Store  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._.DS_Store  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00160_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00160_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00160_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00160_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00021_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00021_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00021_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00021_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00155_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00155_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00155_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00155_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00152_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00152_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00152_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00152_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00009_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00009_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00009_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00009_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00172_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00172_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00152_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00152_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00152_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00152_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00172_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00172_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00152_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00152_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00152_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00152_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00141_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00141_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00141_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00141_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00010_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00010_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00010_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00010_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00012_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00012_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00141_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00141_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00141_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00141_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00021_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00021_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00021_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00021_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00182_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00182_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00010_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00010_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00010_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00010_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00010_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00010_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00010_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00010_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00150_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00150_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00167_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00167_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00150_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00150_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00150_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00150_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00150_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00150_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00150_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00150_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00150_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00150_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00009_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00009_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00009_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00009_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00155_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00155_rot270.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00155_rot270.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00155_rot270.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00025_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00025_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00172_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00172_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/train/frame_00172_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/train/._frame_00172_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00009_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00009_rot0.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00009_rot0.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00009_rot0.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00141_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00141_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00141_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00141_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00021_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00021_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00021_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00021_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00150_rot180.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00150_rot180.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00150_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00150_rot180.jpg  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00009_rot90.xml  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00009_rot90.xml  \n",
      "  inflating: extracted_dataset_5/sheep_video_02/val/frame_00009_rot90.jpg  \n",
      "  inflating: extracted_dataset_5/__MACOSX/sheep_video_02/val/._frame_00009_rot90.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Extract dataset .zip - Modify dataset filepath below to change file\n",
    "# # Dataset 1\n",
    "# !curl -L \"https://universe.roboflow.com/ds/Cztxji5jQE?key=be41mb5A7U\" > dataset_1.zip\n",
    "# !ls\n",
    "# !unzip dataset_1.zip -d extracted_dataset_1\n",
    "# # Dataset 2\n",
    "# !curl -L \"https://universe.roboflow.com/ds/cAuiHPSNbg?key=mqvek1KwT2\" > dataset_2.zip\n",
    "# !ls\n",
    "# !unzip dataset_1.zip -d extracted_dataset_2\n",
    "# Dataset 3 \n",
    "# !curl -L \"https://universe.roboflow.com/ds/sUnQ0kXD5f?key=i403GGBAZM\" > dataset_3.zip\n",
    "# !ls\n",
    "# !unzip dataset_1.zip -d extracted_dataset_3\n",
    "# Custom dataset upload extractions\n",
    "#!unzip dataset_4.zip -d extracted_dataset_4\n",
    "!unzip dataset_5.zip -d extracted_dataset_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJqlGuCJgOaK"
   },
   "source": [
    "### Faster-RCNN Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhS36pJhxK0m"
   },
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6k5NGr7PxM1w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsoxdoUnxUL8"
   },
   "source": [
    "Initialise directory vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F155X-4SxYRY"
   },
   "outputs": [],
   "source": [
    "train_dir = 'extracted_dataset/train'\n",
    "valid_dir = 'extracted_dataset/valid'\n",
    "test_dir = 'extracted_dataset/test'\n",
    "train_dir_2 = 'extracted_dataset_2/train'\n",
    "valid_dir_2 = 'extracted_dataset_2/valid'\n",
    "test_dir_2 = 'extracted_dataset_2/test'\n",
    "train_dir_3 = 'extracted_dataset_3/train'\n",
    "valid_dir_3 = 'extracted_dataset_3/valid'\n",
    "test_dir_3 = 'extracted_dataset_3/test'\n",
    "train_dir_4 = 'extracted_dataset_4/sheep_video_01/train'\n",
    "valid_dir_4 = 'extracted_dataset_4/sheep_video_01/valid'\n",
    "test_dir_4 = 'extracted_dataset_4/sheep_video_01/test'\n",
    "train_dir_5 = 'extracted_dataset_5/sheep_video_02/train'\n",
    "valid_dir_5 = 'extracted_dataset_5/sheep_video_02/valid'\n",
    "test_dir_5 = 'extracted_dataset_5/sheep_video_02/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCKevD6i0Zij"
   },
   "source": [
    "Initialise class vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VmXPwlWf0be-"
   },
   "outputs": [],
   "source": [
    "classes = ['__background__', 'sheep']\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4GMdXvZ04CL"
   },
   "source": [
    "Define VOCDataset class to parse VOC xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pKJo6T1F1CTl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a VOC (Visual Object Classes) dataset from formated object detection directories at a given path.\n",
    "\"\"\"\n",
    "class VOCDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Initialiser for the VOCDataset class.\n",
    "\n",
    "    Args:\n",
    "      root_dir: the specified target path to parse the VOC data.\n",
    "      transforms: the transforms to apply to the images.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        # Set root directory where images and annotations are stored\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Get list of all image filenames ending with .jpg\n",
    "        self.images = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
    "\n",
    "        # Sort filenames to maintain consistent ordering\n",
    "        self.images.sort()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function to get an image and its associated object data for a given index.\n",
    "\n",
    "    Args:\n",
    "      idx: the index of the data to get.\n",
    "    \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image filename\n",
    "        img_name = self.images[idx]\n",
    "\n",
    "        # Build full paths for the image and its corresponding annotation file\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        xml_path = img_path.replace('.jpg', '.xml')\n",
    "\n",
    "        # Load image and convert to RGB\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Parse XML annotation to get bounding boxes and labels\n",
    "        boxes, labels = self.parse_voc_xml(xml_path)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Build target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        # Apply transforms if any\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        else:\n",
    "            img = F.to_tensor(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    \"\"\"A helper to return the length of the dataset.\"\"\"\n",
    "    def __len__(self):\n",
    "        # Return total number of images\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    A helper function to parse an individual VOC formatted XML file.\n",
    "\n",
    "    Args:\n",
    "      xml_file: the path to the XML file to parse.\n",
    "    \"\"\"\n",
    "    def parse_voc_xml(self, xml_file):\n",
    "        # Parse the XML annotation file using ElementTree\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Initialize lists to store bounding boxes and labels\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        # Loop over all object elements in the XML\n",
    "        for obj in root.findall(\"object\"):\n",
    "            # Get the object class name\n",
    "            label = obj.find(\"name\").text\n",
    "\n",
    "            # Skip labels that are not in the defined CLASSES list\n",
    "            if label not in classes:\n",
    "                continue\n",
    "\n",
    "            # Convert label name to its corresponding index in CLASSES\n",
    "            labels.append(classes.index(label))\n",
    "\n",
    "            # Extract the bounding box coordinates from the XML\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            box = [\n",
    "                float(bbox.find(\"xmin\").text),  # left\n",
    "                float(bbox.find(\"ymin\").text),  # top\n",
    "                float(bbox.find(\"xmax\").text),  # right\n",
    "                float(bbox.find(\"ymax\").text)   # bottom\n",
    "            ]\n",
    "            boxes.append(box)\n",
    "\n",
    "        # Return list of bounding boxes and their corresponding labels\n",
    "        return boxes, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcY1cpB14PbU"
   },
   "source": [
    "Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training, test and validaation dataset\n",
    "train_dataset = VOCDataset(train_dir)\n",
    "train_dataset_2 = VOCDataset(train_dir_2)\n",
    "train_dataset_3 = VOCDataset(train_dir_3)\n",
    "train_dataset_4 = VOCDataset(train_dir_4)\n",
    "train_dataset_5 = VOCDataset(train_dir_5)\n",
    "combined_train_dataset = ConcatDataset([train_dataset, train_dataset_2, train_dataset_3, train_dataset_4, train_dataset_5])\n",
    "test_dataset = VOCDataset(test_dir)\n",
    "test_dataset_2 = VOCDataset(test_dir_2)\n",
    "test_dataset_3 = VOCDataset(test_dir_3)\n",
    "test_dataset_4 = VOCDataset(test_dir_4)\n",
    "combined_test_dataset = ConcatDataset([test_dataset, test_dataset_2, test_dataset_3, test_dataset_4])\n",
    "valid_dataset = VOCDataset(valid_dir)\n",
    "valid_dataset_2 = VOCDataset(valid_dir_2)\n",
    "valid_dataset_3 = VOCDataset(valid_dir_3)\n",
    "valid_dataset_4 = VOCDataset(valid_dir_4)\n",
    "combined_valid_dataset = ConcatDataset([valid_dataset, valid_dataset_2, valid_dataset_3, valid_dataset_4])\n",
    "\n",
    "# Set device to run model against\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcY1cpB14PbU"
   },
   "source": [
    "Define CustomFastRCNNPredictor and FRCNNObjectDetector Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oljZjqMm4Y9v"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An CustomFastRCNNPredictor class to be used as the predictor head for the FRCNN model in the FRCNNObjectDetector class.\n",
    "\"\"\"\n",
    "class CustomFastRCNNPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_channels, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        self.bbox_regressor = nn.Linear(in_channels, num_classes * 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.classifier(x)\n",
    "        bbox_deltas = self.bbox_regressor(x)\n",
    "        return scores, bbox_deltas\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "An FRCNNObjectDetector class which can be configured to use Faster-RCNN.\n",
    "\"\"\"\n",
    "class FRCNNObjectDetector():\n",
    "  \"\"\"\n",
    "  Initialiser for the FRCNNObjectDetector class.\n",
    "\n",
    "  Args:\n",
    "    model_name: the name of the current model to train\n",
    "    classes: the classes to detect.\n",
    "    num_classes: the number of classes to detect.\n",
    "    train_dataset: the training dataset.\n",
    "    test_dataset: the testing dataset.\n",
    "    valid_dataset: the validation dataset.\n",
    "    epochs: the number of epochs to train the model for.\n",
    "  \"\"\"\n",
    "  def __init__(self, model_name, classes, num_classes, train_dataset, test_dataset, valid_dataset, device, epochs=10):\n",
    "    self.model_name = model_name\n",
    "    self.classes = classes\n",
    "    self.num_classes = num_classes\n",
    "    self.train_dataset = train_dataset\n",
    "    self.test_dataset = test_dataset\n",
    "    self.valid_dataset = valid_dataset\n",
    "    self.device = device\n",
    "    self.epochs = epochs\n",
    "    self.model = self.get_model()\n",
    "    self.start_epoch = 0\n",
    "\n",
    "  \"\"\"Function to train the initialised model for the object detector.\"\"\"\n",
    "  def train_model(self, model_load_filepath=None):\n",
    "    print(f\"####### Training Object Detection model - classes: {self.classes} #######\")\n",
    "\n",
    "    # Create a DataLoader with custom collate function for handling variable-size targets\n",
    "    data_loader = DataLoader(\n",
    "        self.train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "        \n",
    "    # Move model to run on GPU if available\n",
    "    model = self.model.to(self.device)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(self.model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Load model checkpoint if file is given\n",
    "    if model_load_filepath:\n",
    "        optimizer, self.start_epoch = self.load_model(model_load_filepath, optimizer)\n",
    "    else:\n",
    "        self.start_epoch = 0\n",
    "      \n",
    "    #### Logging and Checkpointing Settings\n",
    "    print_every = 1        # Print loss every N batches\n",
    "    save_every = 1         # Save model every N epochs\n",
    "    val_every = 1          # Validate every N epochs\n",
    "    save_dir = 'models/object_detection' # Path to save model checkpoints\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize logging lists\n",
    "    epoch_losses = []\n",
    "    iteration_losses = []\n",
    "    val_maps = []\n",
    "\n",
    "    # Start training loop\n",
    "    for epoch in range(self.start_epoch, self.epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0  # Track total loss for the epoch\n",
    "\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "\n",
    "            # Create new clean images and targets lists\n",
    "            clean_images = []\n",
    "            clean_targets = []\n",
    "\n",
    "            for img, tgt in zip(images, targets):\n",
    "                img, tgt = self.remove_invalid_boxes(img, tgt)\n",
    "                if img is not None and tgt is not None:\n",
    "                    clean_images.append(img.to(self.device))\n",
    "                    clean_targets.append({k: v.to(self.device) for k, v in tgt.items()})\n",
    "\n",
    "            # Move all images and targets to the selected device\n",
    "            images = [img.to(self.device) for img in clean_images]\n",
    "            targets = [{k: v.to(self.device) for k, v in t.items()} for t in clean_targets]\n",
    "\n",
    "            # Get the loss dict from the model\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Combine all losses into a single scalar\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss and append to losses per iteration\n",
    "            total_loss += losses.item()\n",
    "            iteration_losses.append(losses.item())\n",
    "            # Print batch loss every few iterations\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f\"  [Epoch {epoch+1}, Iter {i+1}] Loss: {losses.item():.4f}\")\n",
    "\n",
    "        # Print total loss at the end of the epoch and store losses for loss curve\n",
    "        print(f\"Epoch [{epoch+1}/{self.epochs}], Total Loss: {total_loss:.4f}\")\n",
    "        epoch_losses.append(total_loss)\n",
    "\n",
    "        if (epoch+1) % val_every == 0:\n",
    "          # Evaluate on validation set\n",
    "          val_results = self.evaluate_map(model, self.valid_dataset)\n",
    "          val_map = val_results[\"map\"].item()\n",
    "          val_maps.append(val_map)\n",
    "          print(f\"📈 Validation mAP at epoch {epoch+1}: {val_map:.4f}\")\n",
    "\n",
    "        # Save checkpoint for loss every few epochs\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "          self.save_model(model, optimizer, epoch+1, filename=str(os.path.join(save_dir, f\"{self.model_name}_checkpoint_epoch_{epoch+1}.pth\")))\n",
    "\n",
    "    # save final model after training is complete\n",
    "    self.save_model(model, optimizer, epoch+1, filename=str(os.path.join(save_dir,f\"{self.model_name}_final_epoch_{epoch+1}.pth\")))\n",
    "\n",
    "    # plot training metrics using matplot\n",
    "    self.plot_training_metrics(epoch_losses, val_maps)\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  Function to test the trained model against the test dataset and print result metrics.\n",
    "  \n",
    "  Args:\n",
    "      dataset_test_type: the type of dataset to test against, can be eithr 'test' or 'validate'. Defaults to 'validate'.\n",
    "  \"\"\"\n",
    "  def test_model(self, dataset_test_type='validate'):\n",
    "\n",
    "    # Make sure dataset_test_type input is correct\n",
    "    if dataset_test_type not in ['validate', 'test']:\n",
    "        raise Exception(\"Please select either 'validate' or 'test' as dataset_test_type.\")\n",
    "\n",
    "    # Set selected dataset\n",
    "    dataset = self.valid_dataset if dataset_test_type == 'validate' else self.test_dataset \n",
    "      \n",
    "    # Create a DataLoader for the test set\n",
    "    test_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,  # No shuffling needed for evaluation\n",
    "        collate_fn=lambda x: tuple(zip(*x))  # Handle varying-size targets\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_results = self.evaluate_map(self.model, dataset)\n",
    "    \n",
    "    # Print overall mAP/mAR results for the test set\n",
    "    print(\"\\n📊 Test set mAP/mAR results:\")\n",
    "    for k, v in test_results.items():\n",
    "        # Convert torch tensors to NumPy arrays for cleaner display\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            print(f\"{k}: {v.numpy()}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "    # Print table header for per-class AP and AR\n",
    "    print(f\"\\nAP / AR per class on {dataset_test_type} dataset\\n\" + \"-\"*73)\n",
    "    print(f\"| {'ID':<3} | {'Class':<20} | {'AP':<18} | {'AR':<18} |\")\n",
    "    print(\"-\"*73)\n",
    "    \n",
    "    # Loop through each class (excluding background)\n",
    "    for i, cls in enumerate(self.classes[1:], 1):\n",
    "        # Get AP and AR for the current class\n",
    "        ap = test_results['map_per_class'][i-1].item()\n",
    "        ar = test_results['mar_100_per_class'][i-1].item()\n",
    "    \n",
    "        # Print table row for the class\n",
    "        print(f\"| {i:<3} | {cls:<20} | {ap:<18.3f} | {ar:<18.3f} |\")\n",
    "    \n",
    "    # Print average AP and AR across all classes\n",
    "    print(\"-\"*73)\n",
    "    print(f\"| {'Avg':<24} | {test_results['map'].item():<18.3f} | {test_results['mar_100'].item():<18.3f} |\")\n",
    "    print(\"-\"*73)\n",
    "\n",
    "    \n",
    "  \"\"\"\n",
    "  Helper function to plot completed training metrics\n",
    "\n",
    "  Args:\n",
    "    epoch_losses: a list of loss metric for each training epoch.\n",
    "    val_maps: a list of mean average precision evaluations on the validation dataset.\n",
    "  \"\"\"  \n",
    "  def plot_training_metrics(self, epoch_losses, val_maps):\n",
    "    print(\"Creating plots for epoch loss and mAP metrics!\")\n",
    "    # Plot training loss per epoch\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(epoch_losses, marker='o')\n",
    "    plt.title(\"Training Loss per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot validation mAP per epoch\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(val_maps, marker='s', color='green')\n",
    "    plt.title(\"Validation mAP per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mAP (IoU=1.0)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "  \"\"\"Helper function to retrieve the selected model type for the object detector.\"\"\"\n",
    "  def get_model(self):\n",
    "    # Define backbone model\n",
    "    backbone = resnet_fpn_backbone('resnet50', weights='DEFAULT')\n",
    "\n",
    "    # Create Faster R-CNN model with custom number of classes\n",
    "    model = FasterRCNN(backbone, num_classes=self.num_classes)\n",
    "\n",
    "    # Get the number of input features for the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one for our number of classes\n",
    "    model.roi_heads.box_predictor = CustomFastRCNNPredictor(in_features, self.num_classes)\n",
    "\n",
    "    # Returns selected backbone model\n",
    "    return model\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  Helper to clean invalid boxes from a dataset as some boxes may have 0 height or width.\n",
    "\n",
    "  Args:\n",
    "    image: the associated .\n",
    "    target: the target annotation boxes to clean.\n",
    "  \"\"\"\n",
    "  def remove_invalid_boxes(self, image, target):\n",
    "      boxes = target[\"boxes\"]\n",
    "\n",
    "      # If there are no boxes, skip this sample\n",
    "      if boxes.numel() == 0 or boxes.shape[0] == 0:\n",
    "          return None, None\n",
    "\n",
    "      # Filter boxes with valid width and height\n",
    "      valid = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "\n",
    "      if not valid.any():\n",
    "          print(\"All boxes invalid, skipping image.\")\n",
    "          return None, None\n",
    "\n",
    "      if not valid.all():\n",
    "          print(f\"Dropping {(~valid).sum().item()} invalid boxes\")\n",
    "\n",
    "      # Apply mask to boxes and labels\n",
    "      target[\"boxes\"] = boxes[valid]\n",
    "      if \"labels\" in target:\n",
    "          target[\"labels\"] = target[\"labels\"][valid]\n",
    "\n",
    "      return image, target\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  Evaluates an object detection model using mean Average Precision (mAP) and\n",
    "  Average Recall (AR) metrics across the specified IOU thresholds.\n",
    "\n",
    "  Args:\n",
    "      model: The trained object detection model to evaluate.\n",
    "      dataset: A dataset of image and target pairs.\n",
    "      iou_thresholds: A list of IoU thresholds to use for mAP and AR calculation. Default is [0.5, 0.75].\n",
    "  \"\"\"\n",
    "  def evaluate_map(self, model, dataset, iou_thresholds=[0.5, 0.75]):\n",
    "      # Set model to evaluation mode\n",
    "      model.eval()\n",
    "\n",
    "      # Lists to store predictions and ground truth for all images\n",
    "      all_preds, all_targets = [], []\n",
    "\n",
    "      # Loop over all images in the dataset\n",
    "      for img, target in dataset:\n",
    "          # Add batch dimension and move image to device\n",
    "          img = img.to(self.device).unsqueeze(0)\n",
    "\n",
    "          # Run inference without gradients\n",
    "          with torch.no_grad():\n",
    "              pred = model(img)[0]\n",
    "\n",
    "          # Filter out predictions with low confidence scores\n",
    "          keep = pred['scores'] > 0.05\n",
    "          pred_boxes = pred['boxes'][keep].cpu()\n",
    "          pred_labels = pred['labels'][keep].cpu()\n",
    "          pred_scores = pred['scores'][keep].cpu()\n",
    "\n",
    "          # Store filtered predictions\n",
    "          all_preds.append({\n",
    "              'boxes': pred_boxes,\n",
    "              'labels': pred_labels,\n",
    "              'scores': pred_scores\n",
    "          })\n",
    "\n",
    "          # Store ground truth (converted to CPU)\n",
    "          all_targets.append({\n",
    "              'boxes': target['boxes'].cpu(),\n",
    "              'labels': target['labels'].cpu()\n",
    "          })\n",
    "\n",
    "      # Compute mAP and AR metrics using predictions and targets\n",
    "      return self.compute_map_ar(all_preds, all_targets, (num_classes - 1))\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  Computes mean Average Precision (mAP) and Average Recall (AR) for a set of predictions\n",
    "  and ground truth targets across all classes.\n",
    "\n",
    "  Args:\n",
    "      preds: A list of predictions for each image.\n",
    "      targets: A list of ground truth annotations for each image.\n",
    "      num_classes: The number of object classes (excluding background).\n",
    "  \"\"\"\n",
    "  def compute_map_ar(self, preds, targets, num_classes):\n",
    "      # Initialize the results dictionary with default values\n",
    "      results = {\n",
    "          'map': 0, 'map_50': 0, 'map_75': 0,\n",
    "          'map_per_class': torch.zeros(num_classes),\n",
    "          'mar_1': 0, 'mar_10': 0, 'mar_100': 0,\n",
    "          'mar_100_per_class': torch.zeros(num_classes),\n",
    "      }\n",
    "\n",
    "      # Lists to hold AP and AR values for each class\n",
    "      aps = [[] for _ in range(num_classes)]\n",
    "      ars = [[] for _ in range(num_classes)]\n",
    "\n",
    "      # Loop through each image's predictions and targets\n",
    "      for pred, target in zip(preds, targets):\n",
    "          # Loop through each class (excluding background)\n",
    "          for class_idx in range(1, num_classes+1):\n",
    "              # Filter boxes by current class\n",
    "              gt_mask = target['labels'] == class_idx\n",
    "              pred_mask = pred['labels'] == class_idx\n",
    "\n",
    "              gt_boxes = target['boxes'][gt_mask]\n",
    "              pred_boxes = pred['boxes'][pred_mask]\n",
    "              pred_scores = pred['scores'][pred_mask]\n",
    "\n",
    "              # Skip if no GT or predictions\n",
    "              if len(gt_boxes) == 0 and len(pred_boxes) == 0:\n",
    "                  continue\n",
    "\n",
    "              # Compute IoUs between predictions and ground truth\n",
    "              ious = box_iou(pred_boxes, gt_boxes) if len(gt_boxes) > 0 and len(pred_boxes) > 0 else torch.zeros((0, 0))\n",
    "\n",
    "              # Initialize true positives (TP) and matched GT indices\n",
    "              tp = torch.zeros(len(pred_boxes))\n",
    "              matched = set()\n",
    "\n",
    "              # Match predictions to ground truth based on IoU > 0.5\n",
    "              for i, row in enumerate(ious):\n",
    "                  max_iou, max_j = torch.max(row, dim=0)\n",
    "                  if max_iou > 0.5 and max_j.item() not in matched:\n",
    "                      tp[i] = 1\n",
    "                      matched.add(max_j.item())\n",
    "\n",
    "              # Compute false positives (FP)\n",
    "              fp = 1 - tp\n",
    "\n",
    "              # Cumulative TP and FP for precision-recall curve\n",
    "              cum_tp = torch.cumsum(tp, dim=0)\n",
    "              cum_fp = torch.cumsum(fp, dim=0)\n",
    "\n",
    "              # Compute recall and precision\n",
    "              recalls = cum_tp / (len(gt_boxes) + 1e-6)\n",
    "              precisions = cum_tp / (cum_tp + cum_fp + 1e-6)\n",
    "\n",
    "              # Compute AP (area under precision-recall curve)\n",
    "              ap = torch.trapz(precisions, recalls) if recalls.numel() > 0 else torch.tensor(0.)\n",
    "              # AR is the max recall value\n",
    "              ar = recalls[-1] if recalls.numel() > 0 else torch.tensor(0.)\n",
    "\n",
    "              # Store per-class AP and AR\n",
    "              aps[class_idx-1].append(ap.item())\n",
    "              ars[class_idx-1].append(ar.item())\n",
    "\n",
    "      # Compute average AP and AR for each class\n",
    "      ap_avg = torch.tensor([np.mean(cls_ap) if cls_ap else 0. for cls_ap in aps])\n",
    "      ar_avg = torch.tensor([np.mean(cls_ar) if cls_ar else 0. for cls_ar in ars])\n",
    "\n",
    "      # Save results\n",
    "      results['map_per_class'] = ap_avg\n",
    "      results['mar_100_per_class'] = ar_avg\n",
    "      results['map'] = ap_avg.mean()\n",
    "      results['map_50'] = ap_avg.mean()\n",
    "      results['map_75'] = ap_avg.mean()\n",
    "      results['mar_100'] = ar_avg.mean()\n",
    "      results['mar_10'] = ar_avg.mean()\n",
    "      results['mar_1'] = ar_avg.mean()\n",
    "\n",
    "      return results\n",
    "\n",
    "\n",
    "  def save_model(self, model, optimizer, epoch, filename=\"checkpoint.pth\"):\n",
    "      \"\"\"\n",
    "      Saves the model and optimizer state for later training or inference.\n",
    "      Args:\n",
    "          model (torch.nn.Module): The model to save.\n",
    "          optimizer (torch.optim.Optimizer): The optimizer used during training.\n",
    "          epoch (int): Current training epoch.\n",
    "          filename (str): File name to save the checkpoint.\n",
    "      \"\"\"\n",
    "      torch.save({\n",
    "          'epoch': epoch,\n",
    "          'model_state_dict': model.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict()\n",
    "      }, filename)\n",
    "      print(f\"✅ Saved model checkpoint to: {filename}\")\n",
    "\n",
    "\n",
    "  def load_model(self, filename, optimizer=None):\n",
    "    \"\"\"\n",
    "    Loads a previously checkpointed model from a .pth file.\n",
    "\n",
    "    Args:\n",
    "        filename: the path/name of the file to load.\n",
    "        optimizer: the optimizer state to load\n",
    "    \"\"\"\n",
    "    print(f\"Loading checkpointed model at filepath: {filename}\")\n",
    "    checkpoint = torch.load(filename, map_location=self.device)\n",
    "\n",
    "    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    self.model.to(self.device)\n",
    "\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    start_epoch = checkpoint.get('epoch', 0)\n",
    "    print(f\"✅ Loaded model. Resuming from epoch {start_epoch}\")\n",
    "    return optimizer, start_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD8OAfqoRJ6T"
   },
   "source": [
    "Initialise and train Faster RCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oAYjqlXVROgc",
    "outputId": "5aea93d6-f6b8-4e47-b9f9-5cbd74d0825c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Training Object Detection model - classes: ['__background__', 'sheep'] #######\n",
      "Loading checkpointed model at filepath: models/object_detection/model_01_20251602_checkpoint_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1322/3623443902.py:448: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model. Resuming from epoch 10\n",
      "Dropping 1 invalid boxes\n",
      "  [Epoch 11, Iter 1] Loss: 0.6321\n",
      "  [Epoch 11, Iter 2] Loss: 0.5730\n",
      "  [Epoch 11, Iter 3] Loss: 0.2680\n",
      "  [Epoch 11, Iter 4] Loss: 0.5271\n",
      "  [Epoch 11, Iter 5] Loss: 0.5805\n",
      "  [Epoch 11, Iter 6] Loss: 0.7106\n",
      "  [Epoch 11, Iter 7] Loss: 0.4873\n",
      "  [Epoch 11, Iter 8] Loss: 0.4874\n",
      "  [Epoch 11, Iter 9] Loss: 0.4690\n",
      "  [Epoch 11, Iter 10] Loss: 0.3952\n",
      "  [Epoch 11, Iter 11] Loss: 0.5419\n",
      "  [Epoch 11, Iter 12] Loss: 0.5275\n",
      "  [Epoch 11, Iter 13] Loss: 0.3062\n",
      "  [Epoch 11, Iter 14] Loss: 0.6410\n",
      "  [Epoch 11, Iter 15] Loss: 0.5160\n",
      "  [Epoch 11, Iter 16] Loss: 0.3291\n",
      "  [Epoch 11, Iter 17] Loss: 0.4550\n",
      "  [Epoch 11, Iter 18] Loss: 0.4159\n",
      "  [Epoch 11, Iter 19] Loss: 0.6077\n",
      "  [Epoch 11, Iter 20] Loss: 0.9595\n",
      "Dropping 1 invalid boxes\n",
      "  [Epoch 11, Iter 21] Loss: 0.4159\n",
      "  [Epoch 11, Iter 22] Loss: 0.5886\n",
      "  [Epoch 11, Iter 23] Loss: 0.4352\n",
      "  [Epoch 11, Iter 24] Loss: 0.5383\n",
      "  [Epoch 11, Iter 25] Loss: 0.6531\n",
      "  [Epoch 11, Iter 26] Loss: 0.7148\n",
      "  [Epoch 11, Iter 27] Loss: 0.5732\n",
      "  [Epoch 11, Iter 28] Loss: 0.5994\n",
      "  [Epoch 11, Iter 29] Loss: 0.6449\n",
      "  [Epoch 11, Iter 30] Loss: 0.4159\n",
      "  [Epoch 11, Iter 31] Loss: 1.0015\n",
      "  [Epoch 11, Iter 32] Loss: 0.5294\n",
      "  [Epoch 11, Iter 33] Loss: 0.5788\n",
      "  [Epoch 11, Iter 34] Loss: 0.6481\n",
      "  [Epoch 11, Iter 35] Loss: 0.3036\n",
      "  [Epoch 11, Iter 36] Loss: 0.2263\n",
      "  [Epoch 11, Iter 37] Loss: 0.4326\n",
      "  [Epoch 11, Iter 38] Loss: 0.6689\n",
      "  [Epoch 11, Iter 39] Loss: 0.4923\n",
      "  [Epoch 11, Iter 40] Loss: 0.7349\n",
      "  [Epoch 11, Iter 41] Loss: 0.5634\n",
      "  [Epoch 11, Iter 42] Loss: 0.6757\n",
      "  [Epoch 11, Iter 43] Loss: 0.6156\n",
      "  [Epoch 11, Iter 44] Loss: 0.4102\n",
      "  [Epoch 11, Iter 45] Loss: 0.7271\n",
      "  [Epoch 11, Iter 46] Loss: 0.1897\n",
      "  [Epoch 11, Iter 47] Loss: 0.4057\n",
      "  [Epoch 11, Iter 48] Loss: 0.5202\n",
      "  [Epoch 11, Iter 49] Loss: 0.5302\n",
      "  [Epoch 11, Iter 50] Loss: 0.4575\n",
      "  [Epoch 11, Iter 51] Loss: 0.2706\n",
      "  [Epoch 11, Iter 52] Loss: 0.5466\n",
      "  [Epoch 11, Iter 53] Loss: 0.6976\n",
      "  [Epoch 11, Iter 54] Loss: 0.4449\n",
      "  [Epoch 11, Iter 55] Loss: 0.4771\n",
      "  [Epoch 11, Iter 56] Loss: 0.5700\n",
      "  [Epoch 11, Iter 57] Loss: 0.5559\n",
      "  [Epoch 11, Iter 58] Loss: 0.4954\n",
      "  [Epoch 11, Iter 59] Loss: 0.5746\n",
      "  [Epoch 11, Iter 60] Loss: 0.4075\n",
      "  [Epoch 11, Iter 61] Loss: 0.4162\n",
      "  [Epoch 11, Iter 62] Loss: 0.6895\n",
      "  [Epoch 11, Iter 63] Loss: 0.3218\n",
      "  [Epoch 11, Iter 64] Loss: 0.6106\n",
      "  [Epoch 11, Iter 65] Loss: 0.7355\n",
      "  [Epoch 11, Iter 66] Loss: 0.4866\n",
      "  [Epoch 11, Iter 67] Loss: 0.6327\n",
      "  [Epoch 11, Iter 68] Loss: 0.2822\n",
      "  [Epoch 11, Iter 69] Loss: 0.7159\n",
      "  [Epoch 11, Iter 70] Loss: 0.5803\n",
      "  [Epoch 11, Iter 71] Loss: 0.7296\n",
      "  [Epoch 11, Iter 72] Loss: 0.5260\n",
      "  [Epoch 11, Iter 73] Loss: 0.4472\n",
      "  [Epoch 11, Iter 74] Loss: 0.7325\n",
      "  [Epoch 11, Iter 75] Loss: 0.4319\n",
      "  [Epoch 11, Iter 76] Loss: 0.6246\n",
      "  [Epoch 11, Iter 77] Loss: 0.5654\n",
      "  [Epoch 11, Iter 78] Loss: 0.5024\n",
      "  [Epoch 11, Iter 79] Loss: 0.6932\n",
      "  [Epoch 11, Iter 80] Loss: 0.4590\n",
      "  [Epoch 11, Iter 81] Loss: 0.4987\n",
      "  [Epoch 11, Iter 82] Loss: 0.8854\n",
      "  [Epoch 11, Iter 83] Loss: 0.7624\n",
      "  [Epoch 11, Iter 84] Loss: 0.5806\n",
      "  [Epoch 11, Iter 85] Loss: 0.7542\n",
      "  [Epoch 11, Iter 86] Loss: 1.9079\n",
      "  [Epoch 11, Iter 87] Loss: 0.4662\n",
      "  [Epoch 11, Iter 88] Loss: 0.7460\n",
      "  [Epoch 11, Iter 89] Loss: 0.2423\n",
      "  [Epoch 11, Iter 90] Loss: 0.3151\n",
      "  [Epoch 11, Iter 91] Loss: 0.5350\n",
      "  [Epoch 11, Iter 92] Loss: 0.8164\n",
      "  [Epoch 11, Iter 93] Loss: 0.7945\n"
     ]
    }
   ],
   "source": [
    "#Initialise RCNN Object Detector\n",
    "rcnn_object_detector = FRCNNObjectDetector(\n",
    "    \"model_01_20251602\",\n",
    "    classes,\n",
    "    num_classes,\n",
    "    combined_train_dataset,\n",
    "    combined_test_dataset,\n",
    "    combined_valid_dataset,\n",
    "    device,\n",
    "    14\n",
    ")\n",
    "\n",
    "# Train Faster RCNN Object detector\n",
    "rcnn_object_detector.train_model(model_load_filepath='models/object_detection/model_01_20251602_checkpoint_epoch_10.pth')\n",
    "\n",
    "# Test Faster RCNN Object detector on validation dataset\n",
    "rcnn_object_detector.test_model(dataset_test_type='validate')\n",
    "\n",
    "# Test Faster RCNN Object detector on test dataset\n",
    "rcnn_object_detector.test_model(dataset_test_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Inference of Model on Sample Image #####\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "rcnn_object_detector.model.eval()\n",
    "\n",
    "# Pick an image path from the training set\n",
    "img_path = os.path.join(train_dir, rcnn_object_detector.train_dataset.images[0])\n",
    "\n",
    "# Open the image and convert to RGB\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Convert the image to a tensor and add a batch dimension\n",
    "img_tensor = F.to_tensor(img).unsqueeze(0).to(rcnn_object_detector.device)\n",
    "\n",
    "# Run the model in inference mode without computing gradients\n",
    "with torch.no_grad():\n",
    "    output = rcnn_object_detector.model(img_tensor)[0]  # Get predictions for the first image\n",
    "\n",
    "# Create a figure to display the image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)  # Show the original image\n",
    "ax = plt.gca()   # Get the current axes\n",
    "\n",
    "# Loop through predicted boxes, labels, and scores\n",
    "for box, label, score in zip(output['boxes'], output['labels'], output['scores']):\n",
    "    if score > 0.90:  # Only show predictions above a confidence threshold\n",
    "        # Extract box coordinates\n",
    "        x1, y1, x2, y2 = box.cpu().numpy()\n",
    "\n",
    "        # Draw the bounding box\n",
    "        ax.add_patch(plt.Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1,\n",
    "            edgecolor='red', facecolor='none', linewidth=2\n",
    "        ))\n",
    "\n",
    "        # Draw the label and score\n",
    "        ax.text(\n",
    "            x1, y1,\n",
    "            f\"{classes[label]}: {score:.2f}\",\n",
    "            color='white',\n",
    "            bbox=dict(facecolor='red', alpha=0.5)\n",
    "        )\n",
    "\n",
    "# Hide axis ticks\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
